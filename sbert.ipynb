{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","drive.mount('/content/gdrive')\n","os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/Linking\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DRT0IyaTobpA","executionInfo":{"status":"ok","timestamp":1738827325070,"user_tz":-330,"elapsed":27741,"user":{"displayName":"Kk Hh","userId":"03519339821494306140"}},"outputId":"336013fd-2a9e-480f-dfbc-81735368395b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":33376,"status":"error","timestamp":1738856805904,"user":{"displayName":"Kk Hh","userId":"03519339821494306140"},"user_tz":-330},"id":"eft5CbFBoCU9","outputId":"a049a189-e871-4ca1-bc60-48150c6d21a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Loading data...\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'cve_cwe_mitre_mapped.xlsx'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-de83a814502f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-de83a814502f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;31m# Load and preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cve_cwe_mitre_mapped.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total entries in dataset: {len(df)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cve_cwe_mitre_mapped.xlsx'"]}],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from sentence_transformers import SentenceTransformer\n","from torch import nn\n","from torch.optim import AdamW\n","from tqdm import tqdm\n","from sklearn.metrics import precision_recall_fscore_support\n","import os\n","import json\n","\n","# Configuration\n","config = {\n","    \"model_name\": \"sentence-transformers/all-mpnet-base-v2\",  # Upgraded to more powerful model\n","    \"embedding_size\": 768,  # MPNet's embedding size\n","    \"batch_size\": 32,\n","    \"learning_rate\": 2e-5,\n","    \"epochs\": 50,\n","    \"patience\": 4,\n","    \"threshold\": 0.5,\n","    \"train_size\": 0.70,\n","    \"val_size\": 0.15,\n","    \"test_size\": 0.15,\n","    \"sample_size\": 150000,\n","    \"random_seed\": 42,\n","    \"warmup_steps\": 1000,\n","    \"weight_decay\": 0.01\n","}\n","\n","def process_techniques(technique_str):\n","    \"\"\"\n","    Process the technique string from CSV into a list of technique IDs.\n","    \"\"\"\n","    if pd.isna(technique_str):\n","        return []\n","\n","    techniques = technique_str.split(';')\n","    cleaned_techniques = []\n","\n","    for tech in techniques:\n","        parts = tech.strip().split(' - ')\n","        if parts:\n","            tech_id = parts[0].strip()\n","            if tech_id:\n","                cleaned_techniques.append(tech_id)\n","\n","    return cleaned_techniques\n","\n","class CustomClassifier(nn.Module):\n","    \"\"\"\n","    Enhanced classifier using SBERT embeddings with a more sophisticated architecture\n","    \"\"\"\n","    def __init__(self, num_labels, embedding_size=768):\n","        super().__init__()\n","        self.classifier = nn.Sequential(\n","            nn.Linear(embedding_size, 1024),\n","            nn.LayerNorm(1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(1024, 512),\n","            nn.LayerNorm(512),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(512, num_labels)\n","        )\n","\n","        # Initialize weights\n","        for module in self.classifier.modules():\n","            if isinstance(module, nn.Linear):\n","                nn.init.xavier_uniform_(module.weight)\n","                if module.bias is not None:\n","                    nn.init.constant_(module.bias, 0)\n","\n","    def forward(self, embeddings):\n","        return self.classifier(embeddings)\n","\n","class CVEDataset(Dataset):\n","    \"\"\"\n","    Custom Dataset for CVE data using SBERT with advanced text preprocessing.\n","    \"\"\"\n","    def __init__(self, data, sentence_transformer, technique_list):\n","        self.data = data\n","        self.sentence_transformer = sentence_transformer\n","        self.technique_list = technique_list\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def preprocess_text(self, cve_desc, cwe_desc):\n","        \"\"\"\n","        Enhanced text preprocessing for security domain\n","        \"\"\"\n","        # Combine descriptions with special tokens for better context separation\n","        if pd.notna(cwe_desc):\n","            text = f\"CVE: {cve_desc} [SEP] CWE: {cwe_desc}\"\n","        else:\n","            text = f\"CVE: {cve_desc}\"\n","        return text\n","\n","    def __getitem__(self, idx):\n","        item = self.data.iloc[idx]\n","\n","        # Create label vector\n","        label_vector = torch.zeros(len(self.technique_list))\n","        techniques = process_techniques(item['MITRE_Technique'])\n","\n","        for tech in techniques:\n","            if tech in self.technique_list:\n","                tech_idx = self.technique_list.index(tech)\n","                label_vector[tech_idx] = 1\n","\n","        # Get text embedding with enhanced preprocessing\n","        text = self.preprocess_text(item['CVE_Description'], item['CWE_Description'])\n","        embedding = self.sentence_transformer.encode(text, convert_to_tensor=True)\n","\n","        return {\n","            \"embeddings\": embedding,\n","            \"labels\": label_vector\n","        }\n","\n","class FocalLoss(nn.Module):\n","    \"\"\"\n","    Focal Loss for handling class imbalance\n","    \"\"\"\n","    def __init__(self, alpha=1, gamma=2):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n","        pt = torch.exp(-bce_loss)\n","        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n","        return focal_loss.mean()\n","\n","def evaluate_model(model, loader, device):\n","    \"\"\"\n","    Evaluates the model on the given data loader with detailed metrics\n","    \"\"\"\n","    model.eval()\n","    all_labels = []\n","    all_preds = []\n","    total_loss = 0\n","    criterion = FocalLoss()\n","\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=\"Evaluating\"):\n","            embeddings = batch[\"embeddings\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","\n","            outputs = model(embeddings)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","\n","            preds = torch.sigmoid(outputs) > config[\"threshold\"]\n","\n","            all_labels.append(labels.cpu())\n","            all_preds.append(preds.cpu())\n","\n","    all_labels = torch.cat(all_labels)\n","    all_preds = torch.cat(all_preds)\n","\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        all_labels, all_preds, average=\"micro\", zero_division=0\n","    )\n","\n","    # Calculate macro metrics\n","    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n","        all_labels, all_preds, average=\"macro\", zero_division=0\n","    )\n","\n","    return {\n","        \"micro\": {\"precision\": precision, \"recall\": recall, \"f1\": f1},\n","        \"macro\": {\"precision\": precision_macro, \"recall\": recall_macro, \"f1\": f1_macro},\n","        \"avg_loss\": total_loss / len(loader)\n","    }\n","\n","class EarlyStopping:\n","    \"\"\"\n","    Enhanced early stopping with optional restore\n","    \"\"\"\n","    def __init__(self, patience, delta=0.001):\n","        self.patience = patience\n","        self.counter = 0\n","        self.best_score = None\n","        self.delta = delta\n","        self.best_state = None\n","\n","    def __call__(self, current_score, model_state):\n","        if self.best_score is None or current_score > self.best_score + self.delta:\n","            self.best_score = current_score\n","            self.counter = 0\n","            self.best_state = model_state.copy()\n","            return False\n","        else:\n","            self.counter += 1\n","            return self.counter >= self.patience\n","\n","def main():\n","    # Set device and reproducibility\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    torch.manual_seed(config[\"random_seed\"])\n","    print(f\"Using device: {device}\")\n","\n","    # Load and preprocess data\n","    print(\"Loading data...\")\n","    df = pd.read_excel(\"cve_cwe_mitre_mapped.xlsx\")\n","    print(f\"Total entries in dataset: {len(df)}\")\n","\n","    # Randomly sample entries\n","    sampled_df = df.sample(n=config[\"sample_size\"], random_state=config[\"random_seed\"])\n","    print(f\"Sampled {len(sampled_df)} entries for training\")\n","\n","    # Create list of unique techniques\n","    all_techniques = set()\n","    for techniques in sampled_df['MITRE_Technique'].dropna():\n","        all_techniques.update(process_techniques(techniques))\n","    technique_list = sorted(list(all_techniques))\n","    print(f\"Found {len(technique_list)} unique techniques in sampled data\")\n","\n","    # Initialize SBERT and dataset\n","    sentence_transformer = SentenceTransformer(config[\"model_name\"])\n","    dataset = CVEDataset(sampled_df, sentence_transformer, technique_list)\n","\n","    # Split dataset\n","    total_size = len(dataset)\n","    train_size = int(config[\"train_size\"] * total_size)\n","    val_size = int(config[\"val_size\"] * total_size)\n","    test_size = total_size - train_size - val_size\n","\n","    train_dataset, val_dataset, test_dataset = random_split(\n","        dataset,\n","        [train_size, val_size, test_size],\n","        generator=torch.Generator().manual_seed(config[\"random_seed\"])\n","    )\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])\n","    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"])\n","\n","    # Initialize model and training components\n","    model = CustomClassifier(len(technique_list), config[\"embedding_size\"]).to(device)\n","    optimizer = AdamW(model.parameters(),\n","                     lr=config[\"learning_rate\"],\n","                     weight_decay=config[\"weight_decay\"])\n","    criterion = FocalLoss()\n","    early_stopping = EarlyStopping(patience=config[\"patience\"])\n","\n","    # Create checkpoint directory\n","    os.makedirs(\"checkpoints\", exist_ok=True)\n","    best_model_path = \"checkpoints/best_model.pt\"\n","    best_val_f1 = 0\n","\n","    # Training loop\n","    print(\"Starting training...\")\n","    for epoch in range(config[\"epochs\"]):\n","        model.train()\n","        total_loss = 0\n","\n","        # Training step\n","        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n","        for batch in progress_bar:\n","            optimizer.zero_grad()\n","\n","            embeddings = batch[\"embeddings\"].to(device)  # Move embeddings to GPU\n","            labels = batch[\"labels\"].to(device)  # Move labels to GPU\n","\n","            outputs = model(embeddings)\n","            loss = criterion(outputs, labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            progress_bar.set_postfix({'loss': total_loss / len(train_loader)})\n","\n","        # Validation step\n","        val_metrics = evaluate_model(model, val_loader, device)\n","        print(f\"Epoch {epoch + 1} - Validation metrics:\", val_metrics)\n","\n","        # Save best model\n","        if val_metrics[\"micro\"][\"f1\"] > best_val_f1:\n","            best_val_f1 = val_metrics[\"micro\"][\"f1\"]\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'val_f1': val_metrics[\"micro\"][\"f1\"],\n","                'technique_list': technique_list,\n","                'config': config\n","            }, best_model_path)\n","\n","        # Early stopping check\n","        if early_stopping(val_metrics[\"micro\"][\"f1\"], model.state_dict()):\n","            print(\"Early stopping triggered\")\n","            break\n","\n","    # Load best model and evaluate on test set\n","    checkpoint = torch.load(best_model_path, map_location=device)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    print(f\"Loaded best model from epoch {checkpoint['epoch']} with validation F1: {checkpoint['val_f1']:.4f}\")\n","\n","    test_metrics = evaluate_model(model, test_loader, device)\n","    print(f\"Final test metrics: {test_metrics}\")\n","\n","    # Save results\n","    results = {\n","        'config': config,\n","        'final_test_metrics': test_metrics,\n","        'best_validation_f1': best_val_f1,\n","        'num_techniques': len(technique_list),\n","        'dataset_sizes': {\n","            'original_size': len(df),\n","            'sampled_size': len(sampled_df),\n","            'train': train_size,\n","            'validation': val_size,\n","            'test': test_size\n","        }\n","    }\n","\n","    with open('training_results.json', 'w') as f:\n","        json.dump(results, f, indent=4)\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpNIVi6evern"},"outputs":[],"source":["import torch\n","from torch import nn\n","from sentence_transformers import SentenceTransformer\n","import json\n","from typing import List, Dict\n","\n","class CustomClassifier(nn.Module):\n","    def __init__(self, num_labels, embedding_size=768):\n","        super().__init__()\n","        self.classifier = nn.Sequential(\n","            nn.Linear(embedding_size, 1024),\n","            nn.LayerNorm(1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(1024, 512),\n","            nn.LayerNorm(512),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(512, num_labels)\n","        )\n","\n","        for module in self.classifier.modules():\n","            if isinstance(module, nn.Linear):\n","                nn.init.xavier_uniform_(module.weight)\n","                if module.bias is not None:\n","                    nn.init.constant_(module.bias, 0)\n","\n","    def forward(self, embeddings):\n","        return self.classifier(embeddings)\n","\n","class CVETechniquePredictor:\n","    def __init__(self, model_path=\"checkpoints/best_model.pt\", device=None):\n","        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f\"Using device: {self.device}\")\n","\n","        print(\"Loading checkpoint...\")\n","        self.checkpoint = torch.load(model_path, map_location=self.device)\n","\n","        self.config = self.checkpoint['config']\n","        self.technique_list = self.checkpoint['technique_list']\n","        print(f\"Found {len(self.technique_list)} techniques in model\")\n","\n","        print(\"Initializing SBERT...\")\n","        self.sentence_transformer = SentenceTransformer(self.config[\"model_name\"])\n","        self.sentence_transformer.to(self.device)\n","\n","        print(\"Initializing classifier...\")\n","        self.model = CustomClassifier(\n","            num_labels=len(self.technique_list),\n","            embedding_size=self.config[\"embedding_size\"]\n","        )\n","\n","        print(\"Loading model weights...\")\n","        self.model.load_state_dict(self.checkpoint['model_state_dict'])\n","        self.model.to(self.device)\n","        self.model.eval()\n","        print(\"Model ready for predictions\")\n","\n","    def predict(self, cve_description: str, threshold: float = None) -> Dict[str, float]:\n","        if threshold is None:\n","            threshold = self.config[\"threshold\"]\n","\n","        text = f\"CVE: {cve_description}\"\n","\n","        with torch.no_grad():\n","            embedding = self.sentence_transformer.encode(text, convert_to_tensor=True)\n","            embedding = embedding.to(self.device)\n","\n","            if len(embedding.shape) == 1:\n","                embedding = embedding.unsqueeze(0)\n","\n","            outputs = self.model(embedding)\n","            probabilities = torch.sigmoid(outputs)[0]\n","\n","        predictions = {\n","            self.technique_list[i]: float(prob)\n","            for i, prob in enumerate(probabilities) if prob > 0.2\n","        }\n","\n","        return dict(sorted(predictions.items(), key=lambda x: x[1], reverse=True))\n","\n","def print_predictions(predictions: Dict[str, float], title: str = None, top_k: int = None):\n","    if title:\n","        print(f\"\\n{title}\")\n","        print(\"-\" * 50)\n","\n","    items = list(predictions.items())\n","    if top_k:\n","        items = items[:top_k]\n","\n","    for technique, confidence in items:\n","        print(f\"{technique}: {confidence:.2%}\")\n","\n","# Load CVE data\n","with open('cve_medical_data.json', 'r') as f:\n","    data = json.load(f)\n","\n","def main():\n","    predictor = CVETechniquePredictor()\n","\n","    all_results = []\n","\n","    print(\"\\nAnalyzing CVEs...\")\n","    for entry in data:\n","        cve_id = entry[\"CVE_ID\"]\n","        description = entry[\"Description\"]\n","        predictions = predictor.predict(description)\n","\n","        print_predictions(predictions, f\"Predicted Techniques for {cve_id}\", top_k=5)\n","\n","        # Append results to list\n","        all_results.append({\n","            \"CVE_ID\": cve_id,\n","            \"Predictions\": predictions\n","        })\n","\n","    # Save all results to JSON file\n","    with open(\"output.json\", \"w\") as f:\n","        json.dump(all_results, f, indent=4)\n","\n","    print(\"\\nPredictions saved to output.json\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}